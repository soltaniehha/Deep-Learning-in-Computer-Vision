{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "messy-vs-clean-room.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "18pFn7sBWVQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3cb0bc79-4b11-4333-c99b-44b47449b3c0"
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQWml8rVWWFE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d761e80-de10-4ed0-f539-5ae7cca3ac11"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Available:  []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL_RnTvoWVQR",
        "colab_type": "text"
      },
      "source": [
        "## Downloading the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MKAsjKGWVQR",
        "colab_type": "text"
      },
      "source": [
        "Follow these steps to download and prepare the data:\n",
        "1. Download the train dataset file from https://www.kaggle.com/cdawn1/messy-vs-clean-room?\n",
        "2. Upload it to your Colab environment. After you upload you should be able to see the file by running the `ls` command.\n",
        "3. Unzip it (use the code below)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAt38H4Fmghl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "b2465343-8e5d-4749-a6ec-05cfa23775bd"
      },
      "source": [
        "FILEID='17BB2Ufj-9rTnT9cwZR8fu_sKKGdCXLxM'\n",
        "FILENAME='train.zip'\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=$FILEID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=$FILEID\" -O $FILENAME && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-23 00:16:09--  https://docs.google.com/uc?export=download&confirm=erJp&id=17BB2Ufj-9rTnT9cwZR8fu_sKKGdCXLxM\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.125.139, 108.177.125.100, 108.177.125.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.125.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-b0-docs.googleusercontent.com/docs/securesc/161atmogu2n1s4hm32espbs7qku895kc/0tn15d35nccibvpa9cj3mn9go8pb43hv/1595463300000/12308918870841825745/09451527815795380980Z/17BB2Ufj-9rTnT9cwZR8fu_sKKGdCXLxM?e=download [following]\n",
            "--2020-07-23 00:16:09--  https://doc-08-b0-docs.googleusercontent.com/docs/securesc/161atmogu2n1s4hm32espbs7qku895kc/0tn15d35nccibvpa9cj3mn9go8pb43hv/1595463300000/12308918870841825745/09451527815795380980Z/17BB2Ufj-9rTnT9cwZR8fu_sKKGdCXLxM?e=download\n",
            "Resolving doc-08-b0-docs.googleusercontent.com (doc-08-b0-docs.googleusercontent.com)... 108.177.97.132, 2404:6800:4008:c00::84\n",
            "Connecting to doc-08-b0-docs.googleusercontent.com (doc-08-b0-docs.googleusercontent.com)|108.177.97.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=mknjs2llq9v0a&continue=https://doc-08-b0-docs.googleusercontent.com/docs/securesc/161atmogu2n1s4hm32espbs7qku895kc/0tn15d35nccibvpa9cj3mn9go8pb43hv/1595463300000/12308918870841825745/09451527815795380980Z/17BB2Ufj-9rTnT9cwZR8fu_sKKGdCXLxM?e%3Ddownload&hash=q7l7ibfop9p4j8vgdg6sld60ktcsbl0i [following]\n",
            "--2020-07-23 00:16:09--  https://docs.google.com/nonceSigner?nonce=mknjs2llq9v0a&continue=https://doc-08-b0-docs.googleusercontent.com/docs/securesc/161atmogu2n1s4hm32espbs7qku895kc/0tn15d35nccibvpa9cj3mn9go8pb43hv/1595463300000/12308918870841825745/09451527815795380980Z/17BB2Ufj-9rTnT9cwZR8fu_sKKGdCXLxM?e%3Ddownload&hash=q7l7ibfop9p4j8vgdg6sld60ktcsbl0i\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.125.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-08-b0-docs.googleusercontent.com/docs/securesc/161atmogu2n1s4hm32espbs7qku895kc/0tn15d35nccibvpa9cj3mn9go8pb43hv/1595463300000/12308918870841825745/09451527815795380980Z/17BB2Ufj-9rTnT9cwZR8fu_sKKGdCXLxM?e=download&nonce=mknjs2llq9v0a&user=09451527815795380980Z&hash=aick46blu2q9vv7hprbnq8cmj0b51o51 [following]\n",
            "--2020-07-23 00:16:09--  https://doc-08-b0-docs.googleusercontent.com/docs/securesc/161atmogu2n1s4hm32espbs7qku895kc/0tn15d35nccibvpa9cj3mn9go8pb43hv/1595463300000/12308918870841825745/09451527815795380980Z/17BB2Ufj-9rTnT9cwZR8fu_sKKGdCXLxM?e=download&nonce=mknjs2llq9v0a&user=09451527815795380980Z&hash=aick46blu2q9vv7hprbnq8cmj0b51o51\n",
            "Connecting to doc-08-b0-docs.googleusercontent.com (doc-08-b0-docs.googleusercontent.com)|108.177.97.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘train.zip’\n",
            "\n",
            "train.zip               [  <=>               ]  32.12M  57.5MB/s    in 0.6s    \n",
            "\n",
            "2020-07-23 00:16:11 (57.5 MB/s) - ‘train.zip’ saved [33681249]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uyKGqmjZR3G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "279f9d84-de26-4b11-9ab9-8f4e20f70d73"
      },
      "source": [
        "ls -lh"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 33M\n",
            "drwxr-xr-x 1 root root 4.0K Jul 10 16:29 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n",
            "-rw-r--r-- 1 root root  33M Jul 23 00:16 train.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_95pNVUZa7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q train.zip -d kaggle_original_data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-vcDoYMeftP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r kaggle_original_data/__MACOSX/"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u16kusuQeAQ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5875a59e-0294-4346-ab52-c65f4abb507b"
      },
      "source": [
        "!ls -l kaggle_original_data | head"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Jul 22 20:17 clean\n",
            "drwxr-xr-x 2 root root 4096 Jul 22 20:17 messy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h2qRQnQWVQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import shuffle\n",
        "import os, shutil\n",
        "\n",
        "# list all labels\n",
        "label_dirs = ['clean', 'messy']\n",
        "\n",
        "# The path to the directory where the original\n",
        "# dataset was uncompressed\n",
        "original_dataset_dir = '/content/kaggle_original_data'\n",
        "\n",
        "# The directory where we will\n",
        "# store our smaller dataset\n",
        "base_dir = '/content/messy_vs_clean_room'\n",
        "os.mkdir(base_dir)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "# Directory with our training/validation/test label pictures\n",
        "for target_dir in [train_dir, validation_dir, test_dir]:\n",
        "    for label in label_dirs:\n",
        "        dir = os.path.join(target_dir, label)\n",
        "        os.mkdir(dir)\n",
        "\n",
        "# Copy 70% of each label to train, 15% to valid, and 15% to test directories\n",
        "for label in label_dirs:\n",
        "    fnames = os.listdir(os.path.join(original_dataset_dir, label))\n",
        "    shuffle(fnames)  # shuffling the list\n",
        "    n_img_start_valid = int(len(fnames)*0.7)\n",
        "    n_img_start_test  = int(len(fnames)*0.85)\n",
        "    for fname in fnames[:n_img_start_valid]:  # train\n",
        "        src = os.path.join(original_dataset_dir, label, fname)\n",
        "        dst = os.path.join(train_dir, label, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "    for fname in fnames[n_img_start_valid:n_img_start_test]:  # valid\n",
        "        src = os.path.join(original_dataset_dir, label, fname)\n",
        "        dst = os.path.join(validation_dir, label, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "    for fname in fnames[n_img_start_test:]:  # test\n",
        "        src = os.path.join(original_dataset_dir, label, fname)\n",
        "        dst = os.path.join(test_dir, label, fname)\n",
        "        shutil.copyfile(src, dst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz_EVN0jWVQW",
        "colab_type": "text"
      },
      "source": [
        "As a sanity check, let's count how many pictures we have in each training split (train / validation / test):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5S62youWVQX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "78d3e08f-4482-4156-810d-d48d86934c54"
      },
      "source": [
        "total_train_imgs = 0\n",
        "total_valid_imgs = 0\n",
        "for label in label_dirs:\n",
        "    print('total images for label', label, 'in training:', len(os.listdir(os.path.join(train_dir, label))),\n",
        "                                           'in valid:', len(os.listdir(os.path.join(validation_dir, label))),\n",
        "                                            'in test:', len(os.listdir(os.path.join(test_dir, label))))\n",
        "    total_train_imgs += len(os.listdir(os.path.join(train_dir, label)))\n",
        "    total_valid_imgs += len(os.listdir(os.path.join(validation_dir, label)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total images for label clean in training: 74 in valid: 16 in test: 16\n",
            "total images for label messy in training: 74 in valid: 16 in test: 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKRmuMiuN7PK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "090dd9de-31a5-438b-f928-23339f8a2da5"
      },
      "source": [
        "print('Total number of training images:', total_train_imgs)\n",
        "print('Total number of validation images:', total_valid_imgs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of training images: 148\n",
            "Total number of validation images: 32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nILJEVrsWVQj",
        "colab_type": "text"
      },
      "source": [
        "## Building our network\n",
        "\n",
        "We've already built a small convnet for MNIST in the previous example, so you should be familiar with them. We will reuse the same \n",
        "general structure: our convnet will be a stack of alternated `Conv2D` (with `relu` activation) and `MaxPooling2D` layers.\n",
        "\n",
        "However, since we are dealing with bigger images and a more complex problem, we will make our network accordingly larger: it will have one \n",
        "more `Conv2D` + `MaxPooling2D` stage. This serves both to augment the capacity of the network, and to further reduce the size of the \n",
        "feature maps, so that they aren't overly large when we reach the `Flatten` layer. Here, since we start from inputs of size 150x150 (a \n",
        "somewhat arbitrary choice), we end up with feature maps of size 7x7 right before the `Flatten` layer.\n",
        "\n",
        "Note that the depth of the feature maps is progressively increasing in the network (from 32 to 128), while the size of the feature maps is \n",
        "decreasing (from 148x148 to 7x7). This is a pattern that you will see in almost all convnets.\n",
        "\n",
        "Since we are attacking a binary classification problem, we are ending the network with a single unit (a `Dense` layer of size 1) and a \n",
        "`sigmoid` activation. This unit will encode the probability that the network is looking at one class or the other."
      ]
    }
  ]
}